---
title: "Week4: Clustering and classification"
output:
  html_document:
    theme: cerulean
    author: "Raimo Haikari"
---

# Clustering and classification

## Part I

##### Custom funtions

__cor.test__ calculates correlation p-values

Parameters are:
- mar: dataset 

__positiveCorrelation__ and  __negativeCorrelation__ select one variable and then seek if any of other variables have high positive or negative correlation with selected variable. 

Parameters are:
- M: precalculated correlation matrix
- varname: selected variable
- cLimit: limit of high correlation 

```{r}
rm(list=ls()) # Clear memory

cor.mtest <- function(mat, ...) {
    mat <- as.matrix(mat)
    n <- ncol(mat)
    p.mat<- matrix(NA, n, n)
    diag(p.mat) <- 0
    for (i in 1:(n - 1)) {
        for (j in (i + 1):n) {
            tmp <- cor.test(mat[, i], mat[, j], ...)
            p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
        }
    }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  p.mat
}


negativeCorrelation <- function(M, varname, cLimit = 0.6){
  names(
    which(
      apply(
        as.matrix(sort(M[!rownames(M) %in% varname,varname], decreasing = TRUE)), 
        1, 
        function(x) x < -1 * cLimit
        )
      )
  ) 
}

positiveCorrelation <- function(M, varname, cLimit = 0.6){
  names(
    which(
      apply(
        as.matrix(sort(M[!rownames(M) %in% varname,varname], decreasing = TRUE)), 
        1, 
        function(x) x > cLimit
        )
      )
  ) 
}
```

Load the packages:

```{r,message=FALSE, warning=FALSE}
library(MASS)
library(dplyr)
library(ggplot2)

library(caret)
# library(Rtsne)
library(ggalt)
library(GGally)

library(corrplot)
```

This weeks exercise uses Boston dataset which is a part of MASS -package. Dataset contains information of Housing Values in Suburbs of Boston. It's well known dataset which had been used a lot in different machine learning purposes, escpecially in testing regression models.

Because Dataset is a part of R package, you can find more information about it by typing __?Boston__ in R command line.

Read the data:

```{r}
data("Boston")

options(scipen=999) # disable scientific notation
```

Data has:

- 14 numeric variables
- 2 of variables are discrete [chas and rad]
- 12 of variables are continous
- 506 observations


```{r}
dim(Boston)
str(Boston)

sum(sapply(Boston, function(x){is.double(x)})) # number of continous  variables
sum(sapply(Boston, function(x){!is.double(x)})) # number of discrete variables
```

## Part II

Let's see what our data is made of. I will go through the variables one by one. But first i will calculate correlation matrix and present correlogram. 

##### Correlations

I'll use <a href="http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram">custom function</a> which calculates correlation p-Values. And in the correlogram i drop the correlations that aren't statistically valid. 

Correlogram is organized so that positive correlations lie near diagonal axis. 

Thing that cathes eye is that most of the correlations between chas and other variables seem to be statistically insicnificat. And there are lots of dark circles which means that variables correlate with each other.

When i go through individual variables i will each time print variables that have either positive or negative correlation with active variable.



```{r}
# correlation matrix
cMat <- cor(Boston,use="pairwise.complete.obs")

# matrix of the p-value of the correlation
p.mat <- cor.mtest(Boston)

# Leave blank on no significant coefficient
corrplot(cMat, type="upper", order="hclust", 
         p.mat = p.mat, sig.level = 0.01, insig = "blank")
```

If correlation is __below -0.6__ or __above 0.6__ I consider it to be strong.

```{r}
cLimit = 0.6
```

##### crim

- __per capita crime rate by town__
- right skewed distribution
- 70 % of observations are below 2 
- highest decile consist of values that really stand out of the overall distribution
- in overall towns seem to be safe, but there are some bad neighborhoods
- crim has a high positive correlation with  variable rad (0.6255051)

```{r}
summary(Boston$crim)

quantile(Boston$crim, probs = seq(0, 1, 0.1))

ggplot(Boston, aes(crim)) +
  geom_histogram(binwidth=1,fill="white",colour="black") +
  labs(title="Per capita crime rate by town")

(p = positiveCorrelation(cMat, 'crim', cLimit))
cMat['crim',p]

(p = negativeCorrelation(cMat, 'crim', cLimit))
# cMat['crim',p]
```

##### zn

- __proportion of residential land zoned for lots over 25,000 sq.ft.__ 
- most common value is 0 (~ 0.74 %) 
- lots over 25,000 seem to be quite rare 
- zn has a high positive correlation with variable dis (0.6644082)

```{r}
summary(Boston$zn)

nrow(Boston[which(Boston$zn == 0),])/nrow(Boston)

ggplot(Boston, aes(zn)) +
  geom_histogram(binwidth=1,fill="white",colour="black") +
  labs(title="Proportion of residential land zoned for lots over 25,000 sq.ft",
       x = "Proportion %")


(p = positiveCorrelation(cMat, 'zn', cLimit))
cMat['zn',p]

(p = negativeCorrelation(cMat, 'zn', cLimit))
# cMat['zn',p]
```

##### indus

- __proportion of non-retail business acres per town__
- histogram seems a little bit polarised. Maby the right tail of histogram represents manufacturing areas?
- indus has a high positive correlation with variables nox (0,76), tax (0,72), age (0,64) and  lstat (0,6)
- indus has a high negative correlation with variable dis (-0.71)

Note. Histogram is combined with density plot!

```{r}
summary(Boston$indus)

quantile(Boston$indus, probs = seq(0, 1, 0.1))

ggplot(Boston, aes(x=indus)) + 
    geom_histogram(aes(y=..density..),  
                   binwidth=1,
                   colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666")  +
    labs(title="proportion of non-retail business acres per town",
       y = "Density",
       x = "Proportion %")


p = positiveCorrelation(cMat, 'indus', cLimit)
cMat['indus',p]

(p = negativeCorrelation(cMat, 'indus', cLimit))
cMat['indus',p]
```


##### chas

- __Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)__ 
- binary variable
- 94 % of areas don't bound river
- chas doesn't have either high positive or high negative correlation with other variables

```{r}
summary(Boston$chas)

prop.table(table(Boston$chas))

Boston %>%
  mutate(bounds = ifelse(chas,'Yes','No')) %>%
  ggplot(., aes(bounds)) +
  geom_bar(fill="white",colour="black") + 
  labs(title="Bounds to Charles River")

(p = positiveCorrelation(cMat, 'chas', cLimit))
# cMat['chas',p]

(p = negativeCorrelation(cMat, 'chas', cLimit))
# cMat['chas',p]
```

##### nox

- __nitrogen oxides concentration (parts per 10 million)__
- right skewed distribution with "bumps"
- nox has a high positive correlation with variables indus (0.76),  age (0.73), tax (0,67) and rad (0.61)
- nox has a high nevative correlation with variable  dis (-0.77)

Note. Histogram is combined with density plot!

```{r}
summary(Boston$nox)

ggplot(Boston, aes(x=nox)) + 
    geom_histogram(aes(y=..density..),  
                   binwidth=.02,
                   colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666") 

p = positiveCorrelation(cMat, 'nox', cLimit)
cMat['nox',p]

(p = negativeCorrelation(cMat, 'nox', cLimit))
cMat['nox',p]
```

##### rm

- average number of rooms per dwelling
- distribution is near normal
- typical value is 6
- rh has high positive correlation with variable medv (0.7)
- rh has high negative correlation with variable lstat (-0.61)

Note. Histogram is combined with density plot!

```{r}
summary(Boston$rm)

ggplot(Boston, aes(x=rm)) + 
    geom_histogram(aes(y=..density..),  
                   binwidth=1,
                   colour="black", fill="white") +
    geom_density(alpha=.05, fill="#FF6666") +
    labs(title="Average number of rooms per dwelling",
       x = "Rooms")

(p = positiveCorrelation(cMat, 'rm', cLimit))
cMat['rm',p]

(p = negativeCorrelation(cMat, 'rm', cLimit))
cMat['rm',p]
```

##### age

- proportion of owner-occupied units built prior to 1940.
- dataset was published 1978, so variable tells the porpotion of at least fourty years old houses
- mildly left skewed distribution that has "strong growth" in last decile
- age has a high positive correlation with variables nox (0.73), indus (0.64) and lstat (0.60) 
- age has a high negative correlation with variable dis (-0.75)

```{r}
summary(Boston$age)

quantile(Boston$age, probs = seq(0, 1, 0.1))

ggplot(Boston, aes(x=age)) + 
    geom_histogram(aes(y=..density..),  
                   binwidth=1,
                   colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666") +
    labs(title="Proportion of owner-occupied units built prior to 1940",
        x = "Proportion %", 
        y = "Density")

p = positiveCorrelation(cMat, 'age', cLimit)
cMat['age',p]

(p = negativeCorrelation(cMat, 'age', cLimit))
cMat['age',p]
```


##### dis

- weighted mean of distances to five Boston employment centres. 
- right skewed distribution that is most of areas are near employment centre
- dis has a high correlation with variable zn (0.66)
- dis has a high correlation with variables  indus (-0.71), age (-0.75) and nox (-0.77)

Note. Histogram is combined with density plot!

```{r}
summary(Boston$dis)

ggplot(Boston, aes(x=dis)) + 
    geom_histogram(aes(y=..density..),  
                   binwidth=0.5,
                   colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666") +
    labs(title="weighted mean of distances to five Boston employment centres",
        x = "Distance", 
        y = "Density")

(p = positiveCorrelation(cMat, 'dis', cLimit))
cMat['dis',p]

p = negativeCorrelation(cMat, 'dis', cLimit)
cMat['dis',p]
```

##### rad

- index of accessibility to radial highways
- discrete
- polarised
- rad has a high correlation with variables tax (0.91), crim (0.63) and nox (0.61)

Note. Histogram is combined with density plot!

```{r}
summary(Boston$rad)

ggplot(Boston, aes(x=rad)) + 
    geom_histogram(aes(y=..density..),  
                   binwidth=1,
                   colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666") +
    labs(title="index of accessibility to radial highways",
        x = "Index", 
        y = "Density")

p = positiveCorrelation(cMat, 'rad', cLimit)
cMat['rad',p]

(p = negativeCorrelation(cMat, 'rad', cLimit))
```

##### tax

- full-value property-tax rate per $10,000.
- polarised variable
- tax has a high positive correlation with variables rad (0.91), indus (0.72) and  nox (0.67)

Note. Histogram is combined with density plot!

```{r}
summary(Boston$tax)

ggplot(Boston, aes(x=tax)) + 
    geom_histogram(aes(y=..density..),  
                   binwidth=25,
                   colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666") +
    labs(title="full-value property-tax rate per $10,000",
        x = "property-tax rate", 
        y = "Density")

p = positiveCorrelation(cMat, 'tax', cLimit)
cMat['tax',p]

(p = negativeCorrelation(cMat, 'tax', cLimit))
# cMat['tax',p]

```

##### ptratio

- pupil-teacher ratio by town. 
- left skewed distribution
- mode value is 20 students per teacher
- ptratio has a high correlation with variables rad (0.91), indus (0.72) and nox (0.67)

Note. Histogram is combined with density plot!

```{r}
summary(Boston$ptratio)

ggplot(Boston, aes(x=ptratio)) + 
    geom_histogram(aes(y=..density..),  
                   binwidth=0.5,
                   colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666") +
    labs(title="Pupil-teacher ratio by town",
        x = "pupils per teacher", 
        y = "Density")

p = positiveCorrelation(cMat, 'tax', cLimit)
cMat['tax',p]

(p = negativeCorrelation(cMat, 'tax', cLimit))
# cMat['tax',p]
```

##### black

- 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
- left skewed distribution
- over 80 % of observations have value greater than 364
- black  doesn't have either high positive or high negative correlation with other variables

Note. Histogram is combined with density plot!

```{r}
summary(Boston$black)

quantile(Boston$black, probs = seq(0, 1, 0.1))

ggplot(Boston, aes(x=black)) + 
    geom_histogram(aes(y=..density..),  
                   binwidth=10,
                   colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666") +
    labs(title="1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town",
        x = "proportion of blacks by town", 
        y = "Density")

p = positiveCorrelation(cMat, 'black', cLimit)
# cMat['black',p]

(p = negativeCorrelation(cMat, 'black', cLimit))
# cMat['black',p]

```


##### lstat

- __lower status of the population (percent)__
- right skewed distribution
- there are some poor regions?
- lstat has a high positive correlation with variables indus (0.60) and age (0.60)
- lstat has a high negative correlation with variables rm (-0.61) and medv (-0.74)

Note. Histogram is combined with density plot!

```{r}
summary(Boston$lstat)

quantile(Boston$lstat, probs = seq(0, 1, 0.1))

ggplot(Boston, aes(x=lstat)) + 
    geom_histogram(aes(y=..density..),  
                   binwidth=1,
                   colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666") +
    labs(title="lower status of the population (percent)",
        x = "percentage", 
        y = "Density")

p = positiveCorrelation(cMat, 'lstat', cLimit)
cMat['lstat',p]

p = negativeCorrelation(cMat, 'lstat', cLimit)
cMat['lstat',p]
```

##### medv

- __median value of owner-occupied homes in $1000s__ 
- there are some 20 high value areas that stand out
- otherwise distribution is near normal
- medv has a high positive correlation with variable rm (0,7)
- medv has a high negative correlation with variable lstat (-0.74)

```{r}
summary(Boston$medv)

quantile(Boston$medv, probs = seq(0, 1, 0.1))

tail(sort(Boston$medv),n=20)

ggplot(Boston, aes(x=medv)) + 
    geom_histogram(aes(y=..density..),  
                   binwidth=1,
                   colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666") +
    labs(title="median value of owner-occupied homes in $1000s",
        x = "$1000s", 
        y = "Density")

(p = positiveCorrelation(cMat, 'medv', cLimit))
cMat['medv',p]

(p = negativeCorrelation(cMat, 'medv', cLimit))
cMat['medv',p]
```


##  Part III

Next I will replace the original dataset with standardized dataset. 

After standardization each variable has zero mean and variance of one unit. 

Standardization is common work procedure in data analysis. One main reason for this is that some tasks like dimension reduction give more accurate results when all the variables have same scale. 

Note! LDA which we are going to use later on in this exercise has assumption varibles have same variance.

And if we take a closer look of couple variables, we see what happens.

In original data:
- nox has range 0.385 to 0.8710
- zn has  had range 0 to 100

After standardization:
- nox has range -1.4644 to 2.7296
- zn has range from -0.48724 to 3.8004

```{r}
Boston <- as.data.frame(
  scale(Boston)
)

summary(Boston)
# sd(Boston$zn)
```

Then i will create new category variable based on crime rate values.

Values are cut four bins based on the quantiles. After that will replace original continous variable with created category variable.

```{r}
breaks <- quantile(Boston$crim, probs = seq(0,1,0.25))

crime <- cut(Boston$crim, 
             breaks = breaks, 
             labels = c("low","med_low","med_high","high"),
             include.lowest = TRUE)

# replace continous variable with category variable
Boston$crim <- crime

```

Finally i will divide dataset to train and test sets. 

- Split will be 80/20. 
- 80 % of observations go to the trainset
- Split is done by means on random sampling

```{r}
# set seed for Reproductivit
set.seed(8520)

# choose randomly 80% of the rows
ind <- sample(nrow(Boston),  size = nrow(Boston) * 0.8)

# create train set
train <- Boston[ind,]

# create test set 
test <- Boston[-ind,]
```

## Part IV

It's time to fit a LDA model. Then i'll print the summary of the model.

One thing that chatches eye is the porpotion of trace: first linear discriminant explains 0.95 of the total variance. 

```{r}
lda.fit <- lda(crim ~ ., data = train)

lda.fit
```

Then i will draw the LDA (bi)plot. I'll skip the arrows, because i think they distract more than explain.

Data is polarised; high values define distinct groub. Which probaply explains the high percentage of LD1

Division between: low,med_low,med_high depend more on LD2 and LD3 and there aren't clear class boundaries. Maby dataset doesn't have enough information for LDA or maby task is beyond capablities of linear model..

```{r}
# target classes as numeric
classes <- as.numeric(train$crim)

# plot the lda results
plot(lda.fit,
     col = classes,
     pch = classes,
     dimen = 2)
```

## Part V

Let's see how my model manages with unseen data. 

First I will save the crime categories from the test set and then remove the categorical crime variable from the test dataset.

Then I will predict with the LDA model on the test data.

```{r}
# save crime categories from the test set
categories <- test$crim
# remove variable from test set
test$crim <- NULL
# predict the classes with the LDA model on the test data
pred <- predict(lda.fit, newdata = test)

df = data.frame(obs = categories,
                pred = pred$class)

```

Finally I will analyse results with caret -packages confusionMatrix -function, which will print detailed overview of results.

- Model accuracy is way higher than __No Information rate__. No information rate is so low because there isn't any dominant class in test set.
- In the (bi)Plot of the model we noticed that groub: __High__ was clearly separated of the rest of classess. That separation can also be seen in the prediction results: sensitivity of __Class: high__ is almost 100 %.
- And model has difficulities with __low__, __med_low__ and __med_high__ classess, which didn't have disctinct borders in the (bi)plot either.

```{r}
# for starters let's print the distibution of true values
table(categories)

df = data.frame(obs = categories,
                pred = pred$class)

xtab <- table(df$pred, df$obs)

# summary of results
print(confusionMatrix(xtab))
```

## Part VI

For the last part of this weeks exercises i'll eeload the Boston dataset and standardize the dataset.

```{r}
# clear memory und reload data
rm(list=ls())
data("Boston")

# backup copy of original data
bak <- Boston

# standardize the dataset"
Boston <- as.data.frame(scale(Boston))
```

##### Distance of observations

Let's calculate the distance matrix of observations

```{r}
# calculate the euclidean distance of observations
dist_Boston = as.matrix(dist(Boston, 
                             method = "euclidean"))
```

As a sanity check lets print:

- the most similar observations
- observations that have least common

And according prints distance matrix seems to be ok.

```{r}
copy_of_dist <- dist_Boston

# clear diagonal axis and lower triangle, so that:
# - we have unique distances
# - don't have observations distance with itself, which is always 0
diag(copy_of_dist) <- NA
dist_Boston[lower.tri(copy_of_dist)] <- NA


# - most similar pair
bak[which(copy_of_dist==min(copy_of_dist, na.rm = TRUE), arr.ind = TRUE)[1, ], ]

# - most dissimilar pair
bak[which(copy_of_dist==max(copy_of_dist, na.rm = TRUE), arr.ind = TRUE)[1, ], ]

rm(copy_of_dist)
```

##### K-Means clustering

First i will select the optimal number of clusters.

I'll use so called <a href="https://www.r-bloggers.com/finding-optimal-number-of-clusters/">elbow method</a> where one:

- tries different cluster counts and records each time variance explained as a function of the number of clusters
- plots findings
- looks where the gain starts to slow down

So instead of seeking smallest value we are interested in where the trend chances.

Based on the plot, two seems to be optimal number of clusters.

```{r}
set.seed(123)
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

df <- data.frame(val = twcss,
                 n = 1:length(twcss))

ggplot(df, aes(y = val, x = factor(n), group = 1)) + 
  geom_point() +
  geom_line() +
  labs(x = "Number of clusters", 
       y = "Total within sum of squares")
  


```

No that the number of clusters is fixed. I'll run the clustering once again.

```{r}
Boston$clust <- as.factor(kmeans(Boston, 2)$cluster)

```

I'll visualize results by means of ggpairs plot.

```{r message=FALSE, warning=FALSE, out.width = '100%'}
ggpairs(Boston,
             mapping = aes(col = clust))

Boston %>%
  group_by(clust) %>%
  summarise(n =n(), a = median(age))
```

Because there are "so much" variables, plot becomes disorganized.

But here are some notes:

- G2 has two times more instances than G1

Diagonal axis:
- variable rm G1 and G2 have similar distributions
- variable tax distributions are opposides, G2 is right skewed and G1 left skewed.

So tax values have greater influense when clustering.

And then there are variables like age, where G2 has even distribution and age is extremely left skewed. G2 has houses from various age-groups, while houses in G1 are old. 

Variable dis has opposite behaviour when inspected from clustering viewpoint.

## Bonus I

I'll reload and standardize data. Then I perform  k-means clustering with k value 8. I choose that value because after that the "gain" flattens.

```{r}
# clear memory und reload data
rm(list=ls())
data("Boston")

# standardize the dataset"
Boston <- as.data.frame(scale(Boston))

# clustering 
set.seed(8520)

Boston$clust <- as.factor(kmeans(Boston, 8)$cluster)
#kmB <- kmeans(Boston, centers = 3)
#lda.fit <- lda(kmB$cluster ~ ., data = Boston)


# table(Boston$clust)

```

Then I'll perform the LDA analysis.

Based on the summary, there are two main discriminants that explain most of the clustering results.

```{r}
lda.fit <- lda(clust ~ ., data = Boston)

lda.fit
```

Time to visualize the results with a biplot.

- There are 2 main groups
- variables rad, tax, zn and age are most influential variables 
- groups are quite distict, at least 1,2,4 and 5 form clear areas


```{r out.width = '100%'}
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(Boston$clust)

# plot the lda results
plot(lda.fit,
     col = classes,
     pch = classes,
     dimen = 2)
lda.arrows(lda.fit, myscale = 2)
```

